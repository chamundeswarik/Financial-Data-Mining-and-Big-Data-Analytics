{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " # <h3 align=\"center\"> <font color='green'>Assignment 02</font></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github link:\n",
    "Chamundeswari Koppisetti: https://github.com/chamundeswarik "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#Load the environment\n",
    "env = gym.make('FrozenLake-v0')\n",
    "#Initialize Q-table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Average rewards: 0.0\n",
      "Total Rewards:0.0\n",
      "Learned Q-Table\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Part A\n",
    "\n",
    "#Set learning parameters using tensor constants\n",
    "gamma = tf.constant(0.8, dtype=tf.float32)\n",
    "alpha = tf.constant(0.1, dtype=tf.float32)\n",
    "episodes = 10\n",
    "total = tf.constant(0.0)\n",
    "\n",
    "#Initialize the temp variables used for the calculation of action\n",
    "rand = tf.constant(random.uniform(0,1))\n",
    "epsilon = tf.constant(1, dtype=tf.float32)\n",
    "\n",
    "#Start the tensorflow session and initialize all the global variables\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer()\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        print (i)\n",
    "        # Reset environment and retrieve the first new state\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        # Probability that a uniform random number drawn at each time step has to be smaller for the agent to do exploration\n",
    "        epsilon = tf.constant(tf.divide(1, (1+i)))\n",
    "        \n",
    "        #Start Q-Table Learning\n",
    "        while not done:\n",
    "            \n",
    "            #Choose the action\n",
    "            rand = tf.constant(random.uniform(0,1))\n",
    "            def x1(): \n",
    "                return tf.constant(env.action_space.sample(), dtype=tf.int32)\n",
    "            def x2():\n",
    "                return tf.constant(np.argmax(Q[state]), dtype=tf.int32)\n",
    "            action = (sess.run(tf.cond(tf.less(epsilon, rand), x1, x2)))\n",
    "            \n",
    "            # step(self, action): Step the environment by one timestep. Returns observation, reward, done, info.\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            #Update the Q-Table\n",
    "            a = tf.constant(Q[state, action], dtype=tf.float32)\n",
    "            b = tf.constant(Q[new_state], dtype=tf.float32)\n",
    "            r = tf.constant(reward, dtype=tf.float32)\n",
    "            Q[state, action] += sess.run(tf.multiply(alpha , tf.subtract(tf.add(tf.constant(reward, dtype=tf.float32), tf.multiply(gamma, tf.reduce_max(b))), a)))\n",
    "            \n",
    "            #update the state\n",
    "            state = new_state\n",
    "            \n",
    "            total = (tf.add(tf.cast(total, dtype=tf.float32), r))\n",
    "    \n",
    "    #Print the Learned Q Table and Average rewards\n",
    "    print(\"Average rewards: \" + str(sess.run(total)/episodes))\n",
    "    print(\"Total Rewards:\" + str(sess.run(total)))\n",
    "    print(\"Learned Q-Table\")\n",
    "    print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'episodes' should be large enough(>5000), to make Q-table useful and efficient. This program takes some time to run,   hence only considered 10 episodes to run the code. \n",
    "\n",
    "This program will give poor performance because it creates several new TensorFlow graph nodes per operation. The underlying assumption in TensorFlow is that we'll build a graph once and then call sess.run() on (various parts of) it multiple times. The first time we run a graph is relatively expensive, because TensorFlow has to build various data structures and optimize the execution of the graph across multiple devices.\n",
    "\n",
    "We can also define loss function, then use gradient function to optimize the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************\n",
      "Episodes 9\n",
      "*****************\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "*****************\n",
      "Steps 2\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "#Part B\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "episodes = 10\n",
    "max_steps =1000\n",
    "\n",
    "#Generating the Episodes\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    #Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(i)\n",
    "    clear_output()\n",
    "    print(\"*****************\")\n",
    "    print(\"Episodes\", episode)\n",
    "    print(\"*****************\")\n",
    "    \n",
    "    for step in range(max_steps):        \n",
    "        # Take the action that has the best Q value\n",
    "        action = np.argmax(Q[state])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            print(\"*****************\")\n",
    "            print(\"Steps\", step)\n",
    "            print(\"*****************\")\n",
    "            time.sleep(2)\n",
    "            break\n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
